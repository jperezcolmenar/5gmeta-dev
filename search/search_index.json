{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Producer and Consumer User Guide Introduction This documentation introduces the APIs needed to be able to produce and consume various types of user data to the 5GMETA platform. Background 5GMETA Platform Framework The 5GMETA platform can be seen as a meeting-point between CAM data producers and consumers. It allows the data producers to post data from their data sources (e.g. sensors, camera) so that the data consumers can consume them in streaming mode in order to create data-driven innovative services for their clients. Streaming mode means that the 5GMETA platform doesn\u2019t store data. The data flow from the data producers called Sensors and Devices, through the 5GMETA platform and towards the data consumers which are CAM Services and Applications that rely on CAM data. The Sensors and Devices (S&D) represent the data producers with their data sources generating the data that are sent to the platform. The data sources can be vehicle\u2019s sensors, cameras etc. They communicate with the 5GMETA platform using 5G network. The CAM Services and Applications act as data consumers accessing the data that are made available through the platform. The 5GMETA MEC acts as an intermediate between the Sensors and Devices and the cloud layer. It can be seen as a part of the 5GMETA platform that is deployed close to where the data are produced. Its purpose it to serve the data producers in its vicinity. APIs Description As seen in the high-level platform architecture above, following are some APIs in the Cloud and the MEC layers which are important to know for producing and consuming data. Cloud layer APIs: Discovery API : This API allows MEC infrastructures to publish the credentials and metadata including endpoints, coverage and inventories available computing resources to host data pipelines, indexing all the 5GMETA MEC infrastructures data services, the information to be operated by the cloud, according to third-party application/service developer requests, and the configuration to connect to and from Sensors and Devices. Stream Data : This API makes connections on demand between the cloud platform and the selected MEC platforms from where capture the data according to third-party application/service developer requests, and push all the applicable data samples into the individual topic for the third-party application/service. Dataflow : The Dataflow API stores the request of any third-party application/service developer request in terms of data and geographic production tiles to trigger through the Registration API the data sending from Sensors and Devices, and to create the individual topic for the third-party application/service. The Data Gateway API : This provides to a third-party application/service the endpoint and the topic to produce or consume data limiting the access to legitimate and valid third-party systems. MEC layer APIs: Registration API : The Registration API is a RESTful application working as main interface of communication with the Sensors & Devices. The S&D should connect to this component before publishing messages to receive the DataflowId and Topic needed to publish data using the internal Message Broker. This component also should receive a Keep Alive from the S&D: if the S&D stops sending the Keep alive, the APIs remove its dataflow from the available ones in the platform. The Message Broker : The Message Broker a broker implementing the publish-subscribe pattern. This broker is then connected to the Stream Data in the cloud side of the platform, both to upload data from the S&D to the CAM Services and Applications and to send events from the CAM Services and Applications to the S&D. Video Broker : The Video Broker is a Proxy for video streams coming from the Sensors and Devices. The supported protocols for the streams are V4L, RTP and RTSP. Sensor and Device Interfaces This section describes different aspects on how Sensors & Devices operate and interact with the 5GMETA platform to connect different data sources with a specific production application or service. The communication interfaces described in this section allow the Sensors & Devices to produce data to the platform and receive events from the Third Parties that are consuming data in the platform. A publish-subscribe protocol : the Edge Message Broker implements the publish-subscribe paradigm, so to send messages in the platform is mandatory to support the specific protocols supported by this component (such as MQTT or AMQP). Streaming Standards : Sensors and Devices can produce media streams in RTSP, UDP or WebRTC formats. The data APIs will provide access to WebRTC media streams to CCAM applications. HTTP : the Registration API and the Discovery API in the Edge provide services as a RESTful application, so HTTP can be used to interface with those 5GMETA components is by means of HTTP requests. Data Production Flow The Data Production allows any S&D to publish data in the platform, to be then consumed by the Third Parties. In order to do that, S&Ds can connect to the APIs exposed by the platform to define and register the dataflows it wants to produce and which MEC infrastructure to connect to depending on their geolocation. This interaction is summarized in figure shown below. Event Consumption Flow The Event Consumption allows S&D to receive messages that were generated by CAM Services and Applications. To do so, the S&D has to subscribe to a specific Topic in the Edge Broker, connected to the Stream of data in which the CAM Services and Applications are publishing the events. This interaction is summarized in figure below. CAM Services and Applications Interfaces The communication interfaces described in this section allow to the CAM Services and Applications to register to the platform, consume data from the platform and generate events that will be received from the Sensors & Devices. A publish-subscribe protocol : the Stream Data implements the publish-subscribe paradigm, so to receive messages from the platform is mandatory to support the specific protocols supported by this component (such as KAFKA). HTTP : The Dataflow API, Identity API and cloud Instance API provide services in the form of a RESTful application, so the only way to interface with those 5GMETA components is by means of HTTP requests. WebRTC : Video data from the 5GMETA platform will be sent through the WebRTC protocol. Data Consumption Procedure The Data Consumption allows a CAM Service and Application to receive data from any of the Sensors and Devices publishing in the platform. To do so, a CAM Service and Application can connect to the APIs exposed by the platform to define the dataflows it wants to consume and which pre-processing pipeline. This interaction is summarized in below. Event Generation Procedure The Event Generation allows a CAM Service and Application to generate events and send them to the Sensors and Devices connected to the platform. To do so, a CAM Service and Application connects to the APIs exposed by the platform to generate messages that will be forwarded through the platform to the Sensors and Devices. This interaction is summarized in the figure below. Table of Contents In the next topics we will cover the following APIs and examples: Message Data Broker API - Stream the messages from sensors and devices to the 5G MECs. ActiveMQ Producer and Consumer examples . Stream Data Gateway API - Stream messages from the MECs to the Third Parties. 5GMETA Platform Client - Third party Python client for making requests to 5GMETA Platform. Kafka Event Producer and Consumer examples .","title":"Home"},{"location":"#data-producer-and-consumer-user-guide","text":"","title":"Data Producer and Consumer User Guide"},{"location":"#introduction","text":"This documentation introduces the APIs needed to be able to produce and consume various types of user data to the 5GMETA platform.","title":"Introduction"},{"location":"#background","text":"","title":"Background"},{"location":"#5gmeta-platform-framework","text":"The 5GMETA platform can be seen as a meeting-point between CAM data producers and consumers. It allows the data producers to post data from their data sources (e.g. sensors, camera) so that the data consumers can consume them in streaming mode in order to create data-driven innovative services for their clients. Streaming mode means that the 5GMETA platform doesn\u2019t store data. The data flow from the data producers called Sensors and Devices, through the 5GMETA platform and towards the data consumers which are CAM Services and Applications that rely on CAM data. The Sensors and Devices (S&D) represent the data producers with their data sources generating the data that are sent to the platform. The data sources can be vehicle\u2019s sensors, cameras etc. They communicate with the 5GMETA platform using 5G network. The CAM Services and Applications act as data consumers accessing the data that are made available through the platform. The 5GMETA MEC acts as an intermediate between the Sensors and Devices and the cloud layer. It can be seen as a part of the 5GMETA platform that is deployed close to where the data are produced. Its purpose it to serve the data producers in its vicinity.","title":"5GMETA Platform Framework"},{"location":"#apis-description","text":"As seen in the high-level platform architecture above, following are some APIs in the Cloud and the MEC layers which are important to know for producing and consuming data. Cloud layer APIs: Discovery API : This API allows MEC infrastructures to publish the credentials and metadata including endpoints, coverage and inventories available computing resources to host data pipelines, indexing all the 5GMETA MEC infrastructures data services, the information to be operated by the cloud, according to third-party application/service developer requests, and the configuration to connect to and from Sensors and Devices. Stream Data : This API makes connections on demand between the cloud platform and the selected MEC platforms from where capture the data according to third-party application/service developer requests, and push all the applicable data samples into the individual topic for the third-party application/service. Dataflow : The Dataflow API stores the request of any third-party application/service developer request in terms of data and geographic production tiles to trigger through the Registration API the data sending from Sensors and Devices, and to create the individual topic for the third-party application/service. The Data Gateway API : This provides to a third-party application/service the endpoint and the topic to produce or consume data limiting the access to legitimate and valid third-party systems. MEC layer APIs: Registration API : The Registration API is a RESTful application working as main interface of communication with the Sensors & Devices. The S&D should connect to this component before publishing messages to receive the DataflowId and Topic needed to publish data using the internal Message Broker. This component also should receive a Keep Alive from the S&D: if the S&D stops sending the Keep alive, the APIs remove its dataflow from the available ones in the platform. The Message Broker : The Message Broker a broker implementing the publish-subscribe pattern. This broker is then connected to the Stream Data in the cloud side of the platform, both to upload data from the S&D to the CAM Services and Applications and to send events from the CAM Services and Applications to the S&D. Video Broker : The Video Broker is a Proxy for video streams coming from the Sensors and Devices. The supported protocols for the streams are V4L, RTP and RTSP.","title":"APIs Description"},{"location":"#sensor-and-device-interfaces","text":"This section describes different aspects on how Sensors & Devices operate and interact with the 5GMETA platform to connect different data sources with a specific production application or service. The communication interfaces described in this section allow the Sensors & Devices to produce data to the platform and receive events from the Third Parties that are consuming data in the platform. A publish-subscribe protocol : the Edge Message Broker implements the publish-subscribe paradigm, so to send messages in the platform is mandatory to support the specific protocols supported by this component (such as MQTT or AMQP). Streaming Standards : Sensors and Devices can produce media streams in RTSP, UDP or WebRTC formats. The data APIs will provide access to WebRTC media streams to CCAM applications. HTTP : the Registration API and the Discovery API in the Edge provide services as a RESTful application, so HTTP can be used to interface with those 5GMETA components is by means of HTTP requests.","title":"Sensor and Device Interfaces"},{"location":"#data-production-flow","text":"The Data Production allows any S&D to publish data in the platform, to be then consumed by the Third Parties. In order to do that, S&Ds can connect to the APIs exposed by the platform to define and register the dataflows it wants to produce and which MEC infrastructure to connect to depending on their geolocation. This interaction is summarized in figure shown below.","title":"Data Production Flow"},{"location":"#event-consumption-flow","text":"The Event Consumption allows S&D to receive messages that were generated by CAM Services and Applications. To do so, the S&D has to subscribe to a specific Topic in the Edge Broker, connected to the Stream of data in which the CAM Services and Applications are publishing the events. This interaction is summarized in figure below.","title":"Event Consumption Flow"},{"location":"#cam-services-and-applications-interfaces","text":"The communication interfaces described in this section allow to the CAM Services and Applications to register to the platform, consume data from the platform and generate events that will be received from the Sensors & Devices. A publish-subscribe protocol : the Stream Data implements the publish-subscribe paradigm, so to receive messages from the platform is mandatory to support the specific protocols supported by this component (such as KAFKA). HTTP : The Dataflow API, Identity API and cloud Instance API provide services in the form of a RESTful application, so the only way to interface with those 5GMETA components is by means of HTTP requests. WebRTC : Video data from the 5GMETA platform will be sent through the WebRTC protocol.","title":"CAM Services and Applications Interfaces"},{"location":"#data-consumption-procedure","text":"The Data Consumption allows a CAM Service and Application to receive data from any of the Sensors and Devices publishing in the platform. To do so, a CAM Service and Application can connect to the APIs exposed by the platform to define the dataflows it wants to consume and which pre-processing pipeline. This interaction is summarized in below.","title":"Data Consumption Procedure"},{"location":"#event-generation-procedure","text":"The Event Generation allows a CAM Service and Application to generate events and send them to the Sensors and Devices connected to the platform. To do so, a CAM Service and Application connects to the APIs exposed by the platform to generate messages that will be forwarded through the platform to the Sensors and Devices. This interaction is summarized in the figure below.","title":"Event Generation Procedure"},{"location":"#table-of-contents","text":"In the next topics we will cover the following APIs and examples: Message Data Broker API - Stream the messages from sensors and devices to the 5G MECs. ActiveMQ Producer and Consumer examples . Stream Data Gateway API - Stream messages from the MECs to the Third Parties. 5GMETA Platform Client - Third party Python client for making requests to 5GMETA Platform. Kafka Event Producer and Consumer examples .","title":"Table of Contents"},{"location":"about/","text":"This documentation user guide is created by ICOOR/UNIMORE","title":"About"},{"location":"activemq-example/","text":"Produce and Consumer Examples Description As discussed in the previous section, /activemq_clients provides examples how to produce (cits/image) data from a Sensor&Device to the MEC using python AMQP Qpid Proton reactor API with ActiveMQ. In this section, we will discuss the examples in further detail. Required packages Linux distribution Python version 3.5+ You have successfully installed python-qpid-proton - including any of its dependencies Refering the examples presented, following are some essential parameters while producing data on the 5GMETA platform: source_id : a Unique Identifier to distinguish the source of generated data. tile : Tile of the source from where the data is being generated in form of QuadKey code. e.g. 1230123012301230 (must be 18 chars in [0-3]) datatype : should be one of the allowed datatype [cits, video, image] sub_datatype : depends upon on the datatype e.g. cam, denm, mappem Generic producer structure A typical producer will contain the following fields, as it can be seen in the examples : Discovery Registration API : This API helps you connect your S&Ds and push data to the MEC within a specified tile. Getting tile of the source from its current GPS position: tileTmp = Tile.for_latitude_longitude(latitude=latitude, longitude=longitude, zoom=18) Getting the message-broker access from the MEC within the previous tile : service=\"message-broker\" messageBroker_ip, messageBroker_port = discovery_registration.discover_sb_service(tile,service) Getting AMQP Topic and dataFlowId to push data into the Message Broker : dataflowId, topic = discovery_registration.register(dataflowmetadata,tile) opts.address=\"amqp://\"+username+\":\"+password+\"@\"+messageBroker_ip+\":\"+str(messageBroker_port)+\":/topic://\"+topic jargs = json.dumps(args) Usage Let's take an example of CITS message producer as shown here in sender.py for reference. Pass the latitude and longitude GPS position of your sensor device as shown here in : # Geoposition - Next steps: from GPS device. latitude = 43.3128 longitude = -1.9750 Replace with your metadata in this section shown below. dataflowmetadata = { \"dataTypeInfo\": { \"dataType\": \"cits\", \"dataSubType\": \"json\" }, \"dataInfo\": { \"dataFormat\": \"asn1_jer\", \"dataSampleRate\": 0.0, \"dataflowDirection\": \"upload\", \"extraAttributes\": None, }, \"licenseInfo\": { \"licenseGeolimit\": \"europe\", \"licenseType\": \"profit\" }, \"dataSourceInfo\": { \"sourceTimezone\": 2, \"sourceStratumLevel\": 3, \"sourceId\": 1, \"sourceType\": \"vehicle\", \"sourceLocationInfo\": { \"locationQuadkey\": tile, \"locationCountry\": \"ESP\", \"locationLatitude\": latitude, \"locationLongitude\": longitude } } } Use the sample content.py to generate your messages. Here as you can see the *msgbody contains the CITS message. def messages_generator(num, tile, msgbody='body_cits_message'): messages.clear() #print(\"Sender prepare the messages... \") for i in range(num): props = { \"dataType\": \"cits\", \"dataSubType\": \"cam\", \"dataFormat\":\"asn1_jer\", \"sourceId\": 1, \"locationQuadkey\": tile+str(i%4), \"body_size\": str(sys.getsizeof(msgbody)) } messages.append( Message(body=msgbody, properties=props) ) Steps to run examples Modify address.py to put the appropriate ip , port and topic given by your message broker or run with options: Additional arguments as highlighted below could be parsed to the sender.py : ``` h, --help show this help message and exit a ADDRESS, --address=ADDRESS = address to which messages are sent (default amqp://5gmeta-user:5gmeta-password@192.168.15.34:5673/topic://cits) m MESSAGES, --messages=MESSAGES = number of messages to send (default 100) t TIMEINTERVAL, --timeinterval=TIMEINTERVAL = messages are sent continuosly every time interval seconds (0: send once) (default 10) ``` In one terminal window run wither of the sender scripts depending upon whether you are running your S&D connected to an database or not . You can add additional arguments as shown before: python3 sender.py or python3 sender_with_sd_database_support.py Example output can be seen below: If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip , port and topic given by your message broker ) : Run python3 receiver.py to see the received messages on the subscribed AMQP topic. Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py NB: You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content. Pseudo movement example This example demonstrates data being produced by a moving sensor device. Added some movement around a fixed GPS position in order to simulate movement. Example: cits_send_moving_location.py This way we can move around a MEC that covers tiles: 031333123201033 031333123201211 and a secondary one that covers tiles: 031333123201212 031333123201213 031333123201223 031333123202223 Other resources The initial example for this client is at the link: https://github.com/apache/activemq/tree/main/assembly/src/release/examples/amqp/python - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Data Producers (ActiveMQ)"},{"location":"activemq-example/#produce-and-consumer-examples","text":"","title":"Produce and Consumer Examples"},{"location":"activemq-example/#description","text":"As discussed in the previous section, /activemq_clients provides examples how to produce (cits/image) data from a Sensor&Device to the MEC using python AMQP Qpid Proton reactor API with ActiveMQ. In this section, we will discuss the examples in further detail.","title":"Description"},{"location":"activemq-example/#required-packages","text":"Linux distribution Python version 3.5+ You have successfully installed python-qpid-proton - including any of its dependencies Refering the examples presented, following are some essential parameters while producing data on the 5GMETA platform: source_id : a Unique Identifier to distinguish the source of generated data. tile : Tile of the source from where the data is being generated in form of QuadKey code. e.g. 1230123012301230 (must be 18 chars in [0-3]) datatype : should be one of the allowed datatype [cits, video, image] sub_datatype : depends upon on the datatype e.g. cam, denm, mappem","title":"Required packages"},{"location":"activemq-example/#generic-producer-structure","text":"A typical producer will contain the following fields, as it can be seen in the examples : Discovery Registration API : This API helps you connect your S&Ds and push data to the MEC within a specified tile. Getting tile of the source from its current GPS position: tileTmp = Tile.for_latitude_longitude(latitude=latitude, longitude=longitude, zoom=18) Getting the message-broker access from the MEC within the previous tile : service=\"message-broker\" messageBroker_ip, messageBroker_port = discovery_registration.discover_sb_service(tile,service) Getting AMQP Topic and dataFlowId to push data into the Message Broker : dataflowId, topic = discovery_registration.register(dataflowmetadata,tile) opts.address=\"amqp://\"+username+\":\"+password+\"@\"+messageBroker_ip+\":\"+str(messageBroker_port)+\":/topic://\"+topic jargs = json.dumps(args)","title":"Generic producer structure"},{"location":"activemq-example/#usage","text":"Let's take an example of CITS message producer as shown here in sender.py for reference. Pass the latitude and longitude GPS position of your sensor device as shown here in : # Geoposition - Next steps: from GPS device. latitude = 43.3128 longitude = -1.9750 Replace with your metadata in this section shown below. dataflowmetadata = { \"dataTypeInfo\": { \"dataType\": \"cits\", \"dataSubType\": \"json\" }, \"dataInfo\": { \"dataFormat\": \"asn1_jer\", \"dataSampleRate\": 0.0, \"dataflowDirection\": \"upload\", \"extraAttributes\": None, }, \"licenseInfo\": { \"licenseGeolimit\": \"europe\", \"licenseType\": \"profit\" }, \"dataSourceInfo\": { \"sourceTimezone\": 2, \"sourceStratumLevel\": 3, \"sourceId\": 1, \"sourceType\": \"vehicle\", \"sourceLocationInfo\": { \"locationQuadkey\": tile, \"locationCountry\": \"ESP\", \"locationLatitude\": latitude, \"locationLongitude\": longitude } } } Use the sample content.py to generate your messages. Here as you can see the *msgbody contains the CITS message. def messages_generator(num, tile, msgbody='body_cits_message'): messages.clear() #print(\"Sender prepare the messages... \") for i in range(num): props = { \"dataType\": \"cits\", \"dataSubType\": \"cam\", \"dataFormat\":\"asn1_jer\", \"sourceId\": 1, \"locationQuadkey\": tile+str(i%4), \"body_size\": str(sys.getsizeof(msgbody)) } messages.append( Message(body=msgbody, properties=props) )","title":"Usage"},{"location":"activemq-example/#steps-to-run-examples","text":"Modify address.py to put the appropriate ip , port and topic given by your message broker or run with options: Additional arguments as highlighted below could be parsed to the sender.py : ``` h, --help show this help message and exit a ADDRESS, --address=ADDRESS = address to which messages are sent (default amqp://5gmeta-user:5gmeta-password@192.168.15.34:5673/topic://cits) m MESSAGES, --messages=MESSAGES = number of messages to send (default 100) t TIMEINTERVAL, --timeinterval=TIMEINTERVAL = messages are sent continuosly every time interval seconds (0: send once) (default 10) ``` In one terminal window run wither of the sender scripts depending upon whether you are running your S&D connected to an database or not . You can add additional arguments as shown before: python3 sender.py or python3 sender_with_sd_database_support.py Example output can be seen below: If you want to do some debugging and check if your messages are being sent run in another terminal to receive messages (you have to modify address.py in order to put the appropriate ip , port and topic given by your message broker ) : Run python3 receiver.py to see the received messages on the subscribed AMQP topic. Use the ActiveMQ admin web page to check Messages Enqueued / Dequeued counts match. You can control which AMQP server the examples try to connect to and the messages they send by changing the values in config.py NB: You have to take into account that any modification made on dataflowmetadata must be applied too into the content.py file in order to generate the appropriate content.","title":"Steps to run examples"},{"location":"activemq-example/#pseudo-movement-example","text":"This example demonstrates data being produced by a moving sensor device. Added some movement around a fixed GPS position in order to simulate movement. Example: cits_send_moving_location.py This way we can move around a MEC that covers tiles: 031333123201033 031333123201211 and a secondary one that covers tiles: 031333123201212 031333123201213 031333123201223 031333123202223","title":"Pseudo movement example"},{"location":"activemq-example/#other-resources","text":"The initial example for this client is at the link: https://github.com/apache/activemq/tree/main/assembly/src/release/examples/amqp/python - *https://qpid.apache.org/releases/qpid-proton-0.36.0/proton/python/docs/tutorial.html* - *https://access.redhat.com/documentation/en-us/red_hat_amq/6.3/html/client_connectivity_guide/amqppython*","title":"Other resources"},{"location":"consuming-guide/","text":"5GMETA platform is an IoT oriented platform that produces data from vehicles to be consumed by third party applications in order to get data from several types: CITS json like messages with data extracted from the vehicle (ex: GPS position, etc) Example images jpg Video streaming Guide to consuming data Registering into the application First step to start using 5GMETA platform will be registering on it. Please go to Registration web page and fill the form with the data. Once you have registered you will be able to access the platform and start consuming data. Next you will be guided with some instructions to get that purpouse. Software requirements This guide is oriented to be executed in an Ubuntu 20.04 environment. Extra packages to be installed First of all, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) Also install with pip3: kafka-python numpy python-qpid-proton Platform-client helper application There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform cliente helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line. Client usage Once you have executed the previous command you will be prompted for: * 5GMETA username * 5GMETA password After entering your username/password, client will ask you if you want to: * Consume data from 5GMETA platform * Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to consume data, so we push c. Inmediately client will show which tile have data. And will ask you to select one of them to consume data from. You can stop selecting tiles by pushing q in your keyboard. After selecting tiles client will show which datatype is available in the tiles you have selected. And will ask you to select which datatype do you want to consume. Once selected you will be prompted with the instancetype you can use in the MEC that is managin data from that tile and will be asked to choose one. Once selected you will be prompted with the parameters from 5GMETA platform you have to use in consumer examples Please notice that the parameters showed in that ouput ARE VALID ONLY IF YOU KEEP client.py RUNNING , once you stop it by pressing q those parameters could not be valid for your consumer application. PLEASE DON'T STOP client.py APPLICATION BY PUSHING CTRL-C Consumer examples 5GMETA platform offers some examples to comsume those data. CITS command line consumer image command line consumer video command line consumer Those consumer clients will ask you for the parameters obtained as output in the Client usage section. CITS consumer This client is a Kafka client that will consume CITS data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port Glossary Tile A tile is a square area in the Earth surface defined to define some surface * Tile example location for tile 03133312320110220 Datatype Instancetype","title":"Consuming guide"},{"location":"consuming-guide/#guide-to-consuming-data","text":"","title":"Guide to consuming data"},{"location":"consuming-guide/#registering-into-the-application","text":"First step to start using 5GMETA platform will be registering on it. Please go to Registration web page and fill the form with the data. Once you have registered you will be able to access the platform and start consuming data. Next you will be guided with some instructions to get that purpouse.","title":"Registering into the application"},{"location":"consuming-guide/#software-requirements","text":"This guide is oriented to be executed in an Ubuntu 20.04 environment.","title":"Software requirements"},{"location":"consuming-guide/#extra-packages-to-be-installed","text":"First of all, you will need to install some dependencies (apt-get): python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad (only if you are going to consume video) gstreamer1.0-libav (only if you are going to consume video) python3-gst-1.0 (only if you are going to consume video) Also install with pip3: kafka-python numpy python-qpid-proton","title":"Extra packages to be installed"},{"location":"consuming-guide/#platform-client-helper-application","text":"There is a guided applicaction that will help you to get the apropriate parameters from 5GMETA platform to get the data you need. You can execute it by downloading all content from folder: Platform cliente helper application Once you have donwload that software you can run it by executing: $ python3 client.py in your command line.","title":"Platform-client helper application"},{"location":"consuming-guide/#client-usage","text":"Once you have executed the previous command you will be prompted for: * 5GMETA username * 5GMETA password After entering your username/password, client will ask you if you want to: * Consume data from 5GMETA platform * Produce an event in a vehicle connected to 5GMETA platform In our case we have decided to consume data, so we push c. Inmediately client will show which tile have data. And will ask you to select one of them to consume data from. You can stop selecting tiles by pushing q in your keyboard. After selecting tiles client will show which datatype is available in the tiles you have selected. And will ask you to select which datatype do you want to consume. Once selected you will be prompted with the instancetype you can use in the MEC that is managin data from that tile and will be asked to choose one. Once selected you will be prompted with the parameters from 5GMETA platform you have to use in consumer examples Please notice that the parameters showed in that ouput ARE VALID ONLY IF YOU KEEP client.py RUNNING , once you stop it by pressing q those parameters could not be valid for your consumer application. PLEASE DON'T STOP client.py APPLICATION BY PUSHING CTRL-C","title":"Client usage"},{"location":"consuming-guide/#consumer-examples","text":"5GMETA platform offers some examples to comsume those data. CITS command line consumer image command line consumer video command line consumer Those consumer clients will ask you for the parameters obtained as output in the Client usage section.","title":"Consumer examples"},{"location":"consuming-guide/#cits-consumer","text":"This client is a Kafka client that will consume CITS data from 5GMETA platform and will print on the command line output. It takes as input parameters: Kafka topic Kafka broker address Kafka bootstrap port Kafka schema registry port","title":"CITS consumer"},{"location":"consuming-guide/#glossary","text":"","title":"Glossary"},{"location":"consuming-guide/#tile","text":"A tile is a square area in the Earth surface defined to define some surface * Tile example location for tile 03133312320110220","title":"Tile"},{"location":"consuming-guide/#datatype","text":"","title":"Datatype"},{"location":"consuming-guide/#instancetype","text":"","title":"Instancetype"},{"location":"datasets/","text":"DATASETS TO BE USED DURING THE HACKATHON 5GMETA DATA TYPE DESCRIPTION TILE SAMPLE C-ITS 'Isn't this fun?' 'Isn't this fun?' image \"Isn't this fun?\" \"Isn't this fun?\" Dashes -- is en-dash, --- is em-dash -- is en-dash, --- is em-dash","title":"Dataset guide"},{"location":"datasets/#datasets-to-be-used-during-the-hackathon","text":"5GMETA DATA TYPE DESCRIPTION TILE SAMPLE C-ITS 'Isn't this fun?' 'Isn't this fun?' image \"Isn't this fun?\" \"Isn't this fun?\" Dashes -- is en-dash, --- is em-dash -- is en-dash, --- is em-dash","title":"DATASETS TO BE USED DURING THE HACKATHON"},{"location":"message-databroker/","text":"Message Data Broker Description Message Data Broker is the module used in 5GMETA Cloud platform to stream the messages from Sensors&Devices to the 5G MECs (Multi-access edge computing). Installation Installation requirements can be found in the repository. Clone the repository using git clone https://github.com/5gmetadmin/message-data-broker.git Table of Contents This repository contains the following modules: An ActiveMQ Message Broker ( link ) - which is deployed in the MEC Examples of sender and receiver in python Note: Detailed instructions to build the ActiveMQ source code are available here . Development instructions for Message Data Broker Message Broker can be found in the /src folder Steps to run: sudo docker-compose up -d open http:// :8161 manage ActiveMQ broker admin/admin you can see created topics Required packages pip dependencies python-qpid-proton Examples In the /examples/activemq_clients you will find sample code for different types of AMQP producers as follows: cits_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicles that send messages with some properties attached. image_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicle that sends images (different size, but all the same) cits_receiver_python : a python example to receive messages from AMQP events These producer/consumer examples are described in detail in the following section.","title":"Message Data Broker"},{"location":"message-databroker/#message-data-broker","text":"","title":"Message Data Broker"},{"location":"message-databroker/#description","text":"Message Data Broker is the module used in 5GMETA Cloud platform to stream the messages from Sensors&Devices to the 5G MECs (Multi-access edge computing).","title":"Description"},{"location":"message-databroker/#installation","text":"Installation requirements can be found in the repository. Clone the repository using git clone https://github.com/5gmetadmin/message-data-broker.git","title":"Installation"},{"location":"message-databroker/#table-of-contents","text":"This repository contains the following modules: An ActiveMQ Message Broker ( link ) - which is deployed in the MEC Examples of sender and receiver in python Note: Detailed instructions to build the ActiveMQ source code are available here .","title":"Table of Contents"},{"location":"message-databroker/#development-instructions-for-message-data-broker","text":"Message Broker can be found in the /src folder Steps to run: sudo docker-compose up -d open http:// :8161 manage ActiveMQ broker admin/admin you can see created topics","title":"Development instructions for Message Data Broker"},{"location":"message-databroker/#required-packages","text":"pip dependencies python-qpid-proton","title":"Required packages"},{"location":"message-databroker/#examples","text":"In the /examples/activemq_clients you will find sample code for different types of AMQP producers as follows: cits_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicles that send messages with some properties attached. image_sender_python : a python sender and receiver that run an AMQP sender simulating a number of vehicle that sends images (different size, but all the same) cits_receiver_python : a python example to receive messages from AMQP events These producer/consumer examples are described in detail in the following section.","title":"Examples"},{"location":"stream-avro-producer/","text":"Avro events producer examples This folder contains avro producer example to produce events. Usage Avro Producer config format to produce an event with avro serializer: schema_registry_client = SchemaRegistryClient(schema_registry_conf) avro_serializer = AvroSerializer( schema_registry_client, schema_str, msg_to_dict ) producer_conf = {'bootstrap.servers': args.bootstrap_servers, 'key.serializer': StringSerializer('utf_8'), 'value.serializer': avro_serializer} producer = SerializingProducer(producer_conf) . . . producer.poll(0.0) Usage is: python3 avro_producer_events.py -b bootstrap_ip:9092 -s http://schema_ip:8081 -t topic Example : python3 avro_producer_events.py -b 192.168.15.181:9092 -s http://192.168.15.181:8081 -t events_5gmeta","title":"Avro Events Producer Example"},{"location":"stream-avro-producer/#avro-events-producer-examples","text":"This folder contains avro producer example to produce events.","title":"Avro events producer examples"},{"location":"stream-avro-producer/#usage","text":"Avro Producer config format to produce an event with avro serializer: schema_registry_client = SchemaRegistryClient(schema_registry_conf) avro_serializer = AvroSerializer( schema_registry_client, schema_str, msg_to_dict ) producer_conf = {'bootstrap.servers': args.bootstrap_servers, 'key.serializer': StringSerializer('utf_8'), 'value.serializer': avro_serializer} producer = SerializingProducer(producer_conf) . . . producer.poll(0.0) Usage is: python3 avro_producer_events.py -b bootstrap_ip:9092 -s http://schema_ip:8081 -t topic Example : python3 avro_producer_events.py -b 192.168.15.181:9092 -s http://192.168.15.181:8081 -t events_5gmeta","title":"Usage"},{"location":"stream-datagateway/","text":"Stream Data Gateway Description Stream Data Gateway is the module used in 5GMETA Cloud platform to stream the messages from the MECs to the Third Parties. It is implemented using Kafka ecosystem technologies. This repository can be downloaded from here . Table of Contents The Stream Data Gatawey module contains: Kafka Broker docker-compose and helm chart ( link ). Connectors configuration to retrive the messages from ActiveMQ (src/) Examples of different Kafka Consumers ( link ) Development version for Stream Data Gateway Refer this README for further API details. The dev version can be found here . There is Kafka instance to be deployed locally. Inside src/dev-version/connectors there is CLI to create connectors between AMQP (MEC) and Kafka (Cloud) to push data into Kafka infrastructure. Production version Contains all neccessary informations to deploy Kafka into an AWS infrastructure ( link ). 5GMETA Platform client This is a third party python client for making requests to 5GMETA Platform. Clone the contents of platform-client . Prerequisites Python3 Registered user in the platform. To register use: 5GMETA User Registration Examples In the /examples folder you can find different sample codes for different Kafka producers and consumers as follows: Consumer: Kafka Consumers: working with AVRO (serialization and deserialization). /consumer/cits/cits-consumer.py : a sample python Kafka consumer to receive CITS messages. /consumer/image/image-consumer.py : a sample python Kafka consumer to receive images. /consumer/video/video-consumer.py : a sample python Kafka consumer to receive video streams. Producer: A Kafka producer ( avro_producer_events.py ) as example for the Third Parties message producer. Other: An interactive Avro CLI client to play with Kafka with AVRO ser-deser. Other example of producers Kafka Event message sender Installation Installation requirements can be found on producer/consumer folders respectively. - Clone the repository using git clone https://github.com/5gmetadmin/stream-data-gateway.git Navigate to /examples folder to install dependencies using: pip install -r examples/requirements.txt Kafka producer/consumer examples are described in further detail in the following section.","title":"Stream Data Gateway"},{"location":"stream-datagateway/#stream-data-gateway","text":"","title":"Stream Data Gateway"},{"location":"stream-datagateway/#description","text":"Stream Data Gateway is the module used in 5GMETA Cloud platform to stream the messages from the MECs to the Third Parties. It is implemented using Kafka ecosystem technologies. This repository can be downloaded from here .","title":"Description"},{"location":"stream-datagateway/#table-of-contents","text":"The Stream Data Gatawey module contains: Kafka Broker docker-compose and helm chart ( link ). Connectors configuration to retrive the messages from ActiveMQ (src/) Examples of different Kafka Consumers ( link )","title":"Table of Contents"},{"location":"stream-datagateway/#development-version-for-stream-data-gateway","text":"Refer this README for further API details. The dev version can be found here . There is Kafka instance to be deployed locally. Inside src/dev-version/connectors there is CLI to create connectors between AMQP (MEC) and Kafka (Cloud) to push data into Kafka infrastructure.","title":"Development version for Stream Data Gateway"},{"location":"stream-datagateway/#production-version","text":"Contains all neccessary informations to deploy Kafka into an AWS infrastructure ( link ).","title":"Production version"},{"location":"stream-datagateway/#5gmeta-platform-client","text":"This is a third party python client for making requests to 5GMETA Platform. Clone the contents of platform-client .","title":"5GMETA Platform client"},{"location":"stream-datagateway/#prerequisites","text":"Python3 Registered user in the platform. To register use: 5GMETA User Registration","title":"Prerequisites"},{"location":"stream-datagateway/#examples","text":"In the /examples folder you can find different sample codes for different Kafka producers and consumers as follows:","title":"Examples"},{"location":"stream-datagateway/#consumer","text":"Kafka Consumers: working with AVRO (serialization and deserialization). /consumer/cits/cits-consumer.py : a sample python Kafka consumer to receive CITS messages. /consumer/image/image-consumer.py : a sample python Kafka consumer to receive images. /consumer/video/video-consumer.py : a sample python Kafka consumer to receive video streams.","title":"Consumer:"},{"location":"stream-datagateway/#producer","text":"A Kafka producer ( avro_producer_events.py ) as example for the Third Parties message producer.","title":"Producer:"},{"location":"stream-datagateway/#other","text":"An interactive Avro CLI client to play with Kafka with AVRO ser-deser. Other example of producers Kafka Event message sender","title":"Other:"},{"location":"stream-datagateway/#installation","text":"Installation requirements can be found on producer/consumer folders respectively. - Clone the repository using git clone https://github.com/5gmetadmin/stream-data-gateway.git Navigate to /examples folder to install dependencies using: pip install -r examples/requirements.txt Kafka producer/consumer examples are described in further detail in the following section.","title":"Installation"},{"location":"stream-example/","text":"Consumer and Event Producer Examples In this section, ww walkthrough the Kafka consumer examples. Three sample consumer examples are provided within the repo for cits, video and image datatypes respectively. Required packages to be installed pip3 install -r examples/requirements.txt More dependencies (apt-get): python3-qpid-proton python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad gstreamer1.0-libav python3-gst-1.0 Also install with pip3: kafka-python numpy Usage Consumer instructions Use platform-client to receive appropriate topics and IPs and ports to be used. Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id More examples There are other such examples that are complete and don't need to use external util to get topic and ip/port to access the system. cits/cits-kafka-consumer.py image/image-kafka-consumer.py Avro Producer: producer/avro_producer_events.py","title":"Stream Data Gateway Examples"},{"location":"stream-example/#consumer-and-event-producer-examples","text":"In this section, ww walkthrough the Kafka consumer examples. Three sample consumer examples are provided within the repo for cits, video and image datatypes respectively.","title":"Consumer and Event Producer Examples"},{"location":"stream-example/#required-packages-to-be-installed","text":"pip3 install -r examples/requirements.txt","title":"Required packages to be installed"},{"location":"stream-example/#more-dependencies-apt-get","text":"python3-qpid-proton python3-avro python3-confluent-kafka gstreamer1.0-plugins-bad gstreamer1.0-libav python3-gst-1.0 Also install with pip3: kafka-python numpy","title":"More dependencies (apt-get):"},{"location":"stream-example/#usage","text":"","title":"Usage"},{"location":"stream-example/#consumer-instructions","text":"Use platform-client to receive appropriate topics and IPs and ports to be used. Select the suitable consumer as per the produced data and use as follows: python3 cits-consumer.py topic platformaddress bootstrap_port registry_port or python3 video-consumer.py platformaddress bootstrap_port topic dataflow_id","title":"Consumer instructions"},{"location":"stream-example/#more-examples","text":"There are other such examples that are complete and don't need to use external util to get topic and ip/port to access the system. cits/cits-kafka-consumer.py image/image-kafka-consumer.py Avro Producer: producer/avro_producer_events.py","title":"More examples"}]}